O webcrawling com o comando !crawl faz o crawl de uma url e de seus links. Coloquei um maximo de 15 requests para nao demorar muito. Coloquei tambem um timeout de 15 segundos para cada request.
Utilizei o beautifulSoup
O search eu utilizei uma funcao que fizemos em aula.O search aceita multiplas palavras e faz a soma do tfidf porem o wn_search so aceita uma palavra. Decidi fazer desse jeito para nao ter que loopar por todas os possiveis sinonimos de duas ou mais palavras. 
A terceira entrega ate agora foi a mais legal. A parte de webcrawling deu muito trabalho mas foi muito enriquecedor mexer com a parte de error handling